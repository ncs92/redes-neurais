{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Desenvolvimento de uma rede neural que maximiza os acertos se uma notícia irá ser popular </h3>\n",
    "<h5> Alunas: Elaine Sangali, Ana Frozza </h5> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Introdução </h3>\n",
    "<p> O presente trabalho tem como objetivo projetar uma rede neural artificial que maximize os acertos de uma base de dados de notícias, onde o objetivo é prever se uma notícia obterá sucesso ou não. Para o desenvolvimento da rede será utilizado o Keras. Será testado vários métodos e valores de entradas para tentar obter o maior número de acerto possível. O conteúdo teórico deste trabalho pode ser encontrado no site do </p> \n",
    "[deeplearningbook.com.br](http://deeplearningbook.com.br/capitulos/). \n",
    "\n",
    "<h4> Redes neurais</h4>\n",
    "\n",
    "<p> Uma rede neural tem como objetivo imitar como o cérebro humano aprende. Ela é um mecanismo de aprendizado de máquina muito poderoso, à medida que uma tarefa se torna complicada, há vários perceptrons que formam uma rede que transmitem informações entre si. Um perceptron representa um neurônio. O modelo do Perceptron foi desenvolvido nas décadas de 1950 e 1960 pelo cientista Frank Rosenblatt. Hoje é mais utilizado outros modelos de neurônnios artificias, mas esse seria um modelo básico, como mostra a figura 1, onde o perceptron rece várias entradas, x1; x2; x3 e produz uma única saída binária. </p>\n",
    "\n",
    "![modelo basico perceptron](https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/perceptron.png?w=280) <center> *figura 1 - Modelo básico de um perceptron* </center>\n",
    "\n",
    "<p> No modelo da figura 1, o perceptron possui três entradas, x1; x2; x3, para calcular a saída, Rosenblatt introduziu pesos, w1; w2; w3, números reais que representam a importância das entradas para a saída, assim a entrada x1 possui peso w1, x2 peso w2 e x3 peso w3. A saída do neurônio é binária, 0 ou 1, e é determinada pela soma ponderada, Σjwjxj, menor ou maior do que algum valor limiar (threshold), como mostra a figura 2. </p>\n",
    "\n",
    "![termo algébrico](https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/output.png?w=362) <center> *figura 2 - Modelo algébrico da saída de um perceptron* </center>\n",
    "\n",
    "<p> O modelo da figura 1 seria um modelo básico de um perceptron, mas atualmente é utilizado modelos mais completos que obtem melhores resultados, como o modelo da figura 3. O modelo da figura 1, simplesmente utiliza uma somatória do produto dos pesos com as entradas, mas esse é um modelo muito simples para determinados problemas. No modelo da figura 3, a função de ativação g(.) usará a saída u em uma função, e o resultado do perceptron será a saída da função g. O simbolo Θ representa o viés (bias), que são utilizados no lugar do threshold, os bias são ajustadas da mesma forma que os pesos sinápticos, o bias permite que um neurônio apresente a saída não nula ainda que todas as suas entradas sejam nulas. O bias representa o quão fácil é fazer o perceptron produzir um 1 (disparar). Um perceptron com um viés muito grande tem uma tendência a emitir um 1, e muito pequeno de emitir 0. </p>\n",
    "\n",
    "![modelo matemático neuronio](https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/neuronio.jpeg?resize=300%2C137)  <center> *figura 3 - Modelo de perceptron com bia e função de ativação* </center>\n",
    "\n",
    "<p>O novo modelo utiliza uma função de soma um pouco diferente, ainda é realizado a soma dos produtos dos pesos com as entradas, mas no fim é somado o valor do viés, como mostra a figura 4.</p>\n",
    "\n",
    "![modelo matemático neuronio](https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/formula.png?w=295)  <center> *figura 4 - Modelo algébrico com o viés* </center>\n",
    "\n",
    "<p>Um único perceptron não consegue resolver os problemas grandes, para isso é necessário uma rede de perceptrons. Há três categorias de tipos de redes de perceptrons (Arquiteturas):<p>\n",
    "<ol>\n",
    "    <li>Redes Neurais Feed-Forward: São mais comuns, a primeira camada é a entrada e a última camada é a saída, se houver uma camada oculta entre as duas, é chamado de redes neurais profundas(Deep Learning). A rede calcula uma série de transformação que altera a semelhança entre os casos, as atividades dos neurônios em cada camada são uma função não-linear das atividades na camada anterior. </li>\n",
    "    <li>Redes Recorrentes: Essa rede é utilizada quando para se obter o valor de saída atual é necessário analisar o valor do passado. Essa rede é equivalente a redes muito profundas com uma camada oculta por fatia de tempo; exceto que eles usam os mesmos pesos em cada fatia de tempo e recebem entrada em cada fatia. Eles têm a capacidade de lembrar informações em seu estado oculto por um longo período de tempo, mas é muito difícil treiná-las para usar esse potencial. Podem possuir uma dinâmica complicada, sendo difíceis de treinar, mas são mais biologicamente realistas.</li>\n",
    "    <li>Redes Conectadas Simetricamente: São como as redes recorrentes mas elas possuem o mesmo peso em ambas as direções.  As redes conectadas simetricamente sem unidades ocultas são chamadas de “Redes Hopfield”. As redes conectadas simetricamente com unidades ocultas são chamadas de “Máquinas de Boltzmann”. </li>\n",
    "</ol>    \n",
    "\n",
    "O trabalho atual se enquadra na categoria Redes Neurais Feed-Forward, a arquitetura utilizada é a Redes Multilayer Perceptrons (MLP), a rede MLP é composta por mais de um perceptron, e possui uma camada de entrada, uma de saída que toma uma decisão sobre a entrada, e entre as duas pode haver várias camadas ocultas. O MLP é muito utilizado em problemas de aprendizagem supervisionados, ele treina um conjunto de pares entrada-saída e aprende a modelar a correlação entre as entradas e saídas, no treinamento é realizado o ajuste dos parâmetros, pesos e bias da rede para conseguir minimizar o erro.  O backpropagation é usado para fazer os ajustes dos pesos e de bias em relação ao erro, e o próprio erro pode ser medido de várias maneiras, inclusive pelo erro quadrático médio.\n",
    "\n",
    "<h4> Base de dados: Online News Popularity </h4>\n",
    "\n",
    "A base de dados [Online News Popularity](http://archive.ics.uci.edu/ml/datasets/Online+News+Popularity) possui 60 atributos de várias notícias a ser analisados pela rede neural mais 1 atributo que possui o valor alvo, os atributos possuem o seguinte significado:\n",
    "\n",
    "<ol>\n",
    "<li>url: Url da notícia</li>\n",
    "<li>timedelta: Dias entre a publicação da notícia e a aquisição do conjunto de dados (não-preditiva)</li>\n",
    "<li>n_tokens_title: Quantidade de palavras do título</li>\n",
    "<li>n_tokens_content: Quantidade de palavras do conteúdo</li>\n",
    "<li>n_unique_tokens: Quantidade de palavras únicas no conteúdo</li>\n",
    "<li>n_non_stop_words: Taxa de palavras sem parar no conteúdo</li>\n",
    "<li>n_non_stop_unique_tokens: Quantidade de palavras não únicas no conteúdo</li>\n",
    "<li>num_hrefs: Número de links</li>\n",
    "<li>num_self_hrefs: Número de links para outras notícias publicados pela Mashable </li>\n",
    "<li>num_imgs: Número de imagens</li>\n",
    "<li>num_videos: Número de vídeos </li>\n",
    "<li>average_token_length: Tamanho médio das palavras no conteúdo</li>\n",
    "<li>num_keywords: Número de palavras-chave nos metadados</li>\n",
    "<li>data_channel_is_lifestyle: É o canal de dados 'Lifestyle'?</li>\n",
    "<li>data_channel_is_entertainment: O canal de dados é 'Entretenimento'?</li>\n",
    "<li>data_channel_is_bus: É o canal de dados 'Business'?</li>\n",
    "<li>data_channel_is_socmed: É o canal de dados 'Social Media'?</li>\n",
    "<li>data_channel_is_tech: O canal de dados é 'Tech'? </li>\n",
    "<li>data_channel_is_world: é o canal de dados 'World'? </li>\n",
    "<li>kw_min_min: Pior palavra-chave (min. Compartilhamentos)</li>\n",
    "<li>kw_max_min: Pior palavra-chave (máx. Compartilhamentos)</li>\n",
    "<li>kw_avg_min: Pior palavra-chave (média de compartilhamentos)</li>\n",
    "<li>kw_min_max: Melhor palavra-chave (min. Compartilhamentos)</li>\n",
    "<li>kw_max_max: Melhor palavra-chave (máx. Compartilhamentos)</li>\n",
    "<li>kw_avg_max: Melhor palavra-chave (média de compartilhamentos)</li>\n",
    "<li>kw_min_avg: média palavra-chave (min. partes)</li>\n",
    "<li>kw_max_avg: média palavra-chave (máx. compartilhamentos)</li>\n",
    "<li>kw_avg_avg: média palavra-chave (média de compartilhamentos)</li>\n",
    "<li>self_reference_min_shares: minimo de ações de notícias referenciados em Mashable</li>\n",
    "<li>self_reference_max_shares: máx. ações de notícias referenciados em Mashable</li>\n",
    "<li>self_reference_avg_sharess: média. ações de notícias referenciados em Mashable</li>\n",
    "<li>weekday_is_monday: A notícia foi publicado na segunda-feira?</li>\n",
    "<li>weekday_is_tuesday: A notícia foi publicado em uma terça-feira?</li>\n",
    "<li>weekday_is_wednesday: A notícia foi publicado em uma quarta-feira?</li>\n",
    "<li>weekday_is_thursday: A notícia foi publicado em uma quinta-feira?</li>\n",
    "<li>weekday_is_friday: A notícia foi publicado em uma sexta-feira?</li>\n",
    "<li>weekday_is_saturday: A notícia foi publicado em um sábado?</li>\n",
    "<li>weekday_is_sunday: A notícia foi publicado em um domingo?</li>\n",
    "<li>is_weekend: A notícia foi publicado no final de semana?</li>\n",
    "<li>LDA_00: Proximidade do tópico 0 do LDA</li>\n",
    "<li>LDA_01: Proximidade do tema 1 do LDA</li>\n",
    "<li>LDA_02: Proximidade do tópico 2 do LDA</li>\n",
    "<li>LDA_03: Proximidade do tema 3 do LDA</li>\n",
    "<li>LDA_04: Proximidade do tema 4 do LDA</li>\n",
    "    \n",
    "<li>global_subjectivity: Subjetividade do texto</li>\n",
    "<li>global_sentiment_polarity: polaridade do sentimento de texto</li>\n",
    "<li>global_rate_positive_words: Taxa de palavras positivas no conteúdo</li>\n",
    "<li>global_rate_negative_words: Taxa de palavras negativas no conteúdo</li>\n",
    "<li>rate_positive_words: Taxa de palavras positivas entre tokens não neutros</li>\n",
    "<li>rate_negative_words: Taxa de palavras negativas entre tokens não neutros</li>\n",
    "<li>avg_positive_polarity: média polaridade de palavras positivas</li>\n",
    "<li>min_positive_polarity: min. polaridade de palavras positivas</li>\n",
    "<li>max_positive_polarity: máx. polaridade de palavras positivas</li>\n",
    "<li>avg_negative_polarity: média polaridade de palavras negativas</li>\n",
    "    <li>min_negative_polarity: min. polaridade de palavras negativas</li>\n",
    "<li>max_negative_polarity: máx. polaridade de palavras negativas</li>\n",
    "<li>title_subjectivity: subjetividade do título</li>\n",
    "<li>title_sentiment_polarity: polaridade do título</li>\n",
    "<li>abs_title_subjectivity: Nível de subjetividade absoluta</li>\n",
    "<li>abs_title_sentiment_polarity: nível de polaridade absoluta</li>\n",
    "<li>ações: Número de ações (alvo)</li>\n",
    "</ol>\n",
    "\n",
    "Este conjunto de dados resume um conjunto heterogêneo de características sobre artigos publicados pela Mashable em um período de dois anos. O objetivo é prever o número de compartilhamentos nas redes sociais (popularidade). A base de dados contém 39644 exemplos. A média de compartilhamentos é de 3395, assim, valores abaixo dessa média serão considerados não populares, e valores iguais ou acima dessa média serão considerados populares.\n",
    "\n",
    "<h3> Desenvolvimento </h3>\n",
    "Inicialmente será importado as bibliotecas que serão utilizadas no projeto, e será utilizado um código simples do keras de classificação binária, pois o resultado é binário, ou é popular, ou não é.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Elaine\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#importando bibliotecas necessárias no projeto\n",
    "from sklearn import svm\n",
    "from keras.models import Sequential\n",
    "from keras.utils import plot_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.python.client import device_lib\n",
    "from sklearn.svm import SVC\n",
    "from keras import utils as np_utils\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = csv.reader(open('OnlineNewsPopularity.csv','r'), delimiter=',') #lendo os atributos da base de dados\n",
    "\n",
    "rows = np.array(list(reader))\n",
    "labels = rows[0] #vetor com os labels das caracteristicas\n",
    "\n",
    "X = rows[1:-1, 1:-1] #vetor de caracteristicas\n",
    "Ya = rows[1:-1, -1] #Vetor de resultados\n",
    "\n",
    "Y = []\n",
    "\n",
    "index = 0\n",
    "\n",
    "for y in Ya:\n",
    "    if(int(y) >= 3395):\n",
    "        Y.insert(index, True)        \n",
    "    else:\n",
    "        Y.insert(index, False)\n",
    "    \n",
    "    index += 1\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, stratify=Y) #separando em um conjunto de treino e outro de teste\n",
    "\n",
    "num_input = x_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20812 samples, validate on 6938 samples\n",
      "Epoch 1/50\n",
      "20812/20812 [==============================] - 11s 538us/step - loss: 3.2706 - acc: 0.7971 - val_loss: 3.3268 - val_acc: 0.7936\n",
      "Epoch 2/50\n",
      "20812/20812 [==============================] - 10s 457us/step - loss: 3.2706 - acc: 0.7971 - val_loss: 3.3268 - val_acc: 0.7936\n",
      "Epoch 3/50\n",
      "20812/20812 [==============================] - 10s 460us/step - loss: 3.2706 - acc: 0.7971 - val_loss: 3.3268 - val_acc: 0.7936\n",
      "Epoch 4/50\n",
      "20812/20812 [==============================] - 11s 538us/step - loss: 3.2706 - acc: 0.7971 - val_loss: 3.3268 - val_acc: 0.7936\n",
      "Epoch 5/50\n",
      "20812/20812 [==============================] - 12s 583us/step - loss: 3.2706 - acc: 0.7971 - val_loss: 3.3268 - val_acc: 0.7936\n",
      "Epoch 6/50\n",
      "20812/20812 [==============================] - 12s 599us/step - loss: 3.2706 - acc: 0.7971 - val_loss: 3.3268 - val_acc: 0.7936\n",
      "Epoch 7/50\n",
      "20812/20812 [==============================] - 12s 590us/step - loss: 3.2706 - acc: 0.7971 - val_loss: 3.3268 - val_acc: 0.7936\n",
      "Epoch 8/50\n",
      "20812/20812 [==============================] - 12s 593us/step - loss: 3.2706 - acc: 0.7971 - val_loss: 3.3268 - val_acc: 0.7936\n",
      "Epoch 9/50\n",
      "20812/20812 [==============================] - 14s 669us/step - loss: 3.2706 - acc: 0.7971 - val_loss: 3.3268 - val_acc: 0.7936\n",
      "Epoch 10/50\n",
      "20812/20812 [==============================] - 12s 596us/step - loss: 3.2706 - acc: 0.7971 - val_loss: 3.3268 - val_acc: 0.7936\n",
      "Epoch 11/50\n",
      "19184/20812 [==========================>...] - ETA: 0s - loss: 3.2767 - acc: 0.7967"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-1ce7fc3b434e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m               metrics=['accuracy'])\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mloss_and_metrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=40, activation='relu', input_dim=num_input))\n",
    "model.add(Dense(units=num_input, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_split=0.25, epochs=50, batch_size=16, verbose=1)\n",
    "\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=16)\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
