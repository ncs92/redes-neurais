{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Desenvolvimento de uma rede neural que maximiza os acertos se uma notícia irá ser popular </h3>\n",
    "<h5> Alunas: Elaine Sangali, Ana Frozza </h5> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Introdução </h3>\n",
    "<p> O presente trabalho tem como objetivo projetar uma rede neural artificial que maximize os acertos de uma base de dados de notícias, onde o objetivo é prever se uma notícia obterá sucesso ou não. Para o desenvolvimento da rede será utilizado o Keras. Será testado vários métodos e valores de entradas para tentar obter o maior número de acerto possível. O conteúdo teórico deste trabalho pode ser encontrado no site do </p> \n",
    "[deeplearningbook.com.br](http://deeplearningbook.com.br/capitulos/). \n",
    "\n",
    "<h4> Redes neurais</h4>\n",
    "\n",
    "<p> Uma rede neural tem como objetivo imitar como o cérebro humano aprende. Ela é um mecanismo de aprendizado de máquina muito poderoso, à medida que uma tarefa se torna complicada, há vários perceptrons que formam uma rede que transmitem informações entre si. Um perceptron representa um neurônio. O modelo do Perceptron foi desenvolvido nas décadas de 1950 e 1960 pelo cientista Frank Rosenblatt. Hoje é mais utilizado outros modelos de neurônnios artificias, mas esse seria um modelo básico, como mostra a figura 1, onde o perceptron rece várias entradas, x1; x2; x3 e produz uma única saída binária. </p>\n",
    "\n",
    "![modelo basico perceptron](https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/perceptron.png?w=280) <center> *figura 1 - Modelo básico de um perceptron* </center>\n",
    "\n",
    "<p> No modelo da figura 1, o perceptron possui três entradas, x1; x2; x3, para calcular a saída, Rosenblatt introduziu pesos, w1; w2; w3, números reais que representam a importância das entradas para a saída, assim a entrada x1 possui peso w1, x2 peso w2 e x3 peso w3. A saída do neurônio é binária, 0 ou 1, e é determinada pela soma ponderada, Σjwjxj, menor ou maior do que algum valor limiar (threshold), como mostra a figura 2. </p>\n",
    "\n",
    "![termo algébrico](https://i2.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/output.png?w=362) <center> *figura 2 - Modelo algébrico da saída de um perceptron* </center>\n",
    "\n",
    "<p> O modelo da figura 1 seria um modelo básico de um perceptron, mas atualmente é utilizado modelos mais completos que obtem melhores resultados, como o modelo da figura 3. O modelo da figura 1, simplesmente utiliza uma somatória do produto dos pesos com as entradas, mas esse é um modelo muito simples para determinados problemas. No modelo da figura 3, a função de ativação g(.) usará a saída u em uma função, e o resultado do perceptron será a saída da função g. O simbolo Θ representa o viés (bias), que são utilizados no lugar do threshold, os bias são ajustadas da mesma forma que os pesos sinápticos, o bias permite que um neurônio apresente a saída não nula ainda que todas as suas entradas sejam nulas. O bias representa o quão fácil é fazer o perceptron produzir um 1 (disparar). Um perceptron com um viés muito grande tem uma tendência a emitir um 1, e muito pequeno de emitir 0. </p>\n",
    "\n",
    "![modelo matemático neuronio](https://i0.wp.com/deeplearningbook.com.br/wp-content/uploads/2018/01/neuronio.jpeg?resize=300%2C137)  <center> *figura 3 - Modelo de perceptron com bia e função de ativação* </center>\n",
    "\n",
    "<p>O novo modelo utiliza uma função de soma um pouco diferente, ainda é realizado a soma dos produtos dos pesos com as entradas, mas no fim é somado o valor do viés, como mostra a figura 4.</p>\n",
    "\n",
    "![modelo matemático neuronio](https://i1.wp.com/deeplearningbook.com.br/wp-content/uploads/2017/12/formula.png?w=295)  <center> *figura 4 - Modelo algébrico com o viés* </center>\n",
    "\n",
    "<p>Um único perceptron não consegue resolver os problemas grandes, para isso é necessário uma rede de perceptrons. Há três categorias de tipos de redes de perceptrons (Arquiteturas):<p>\n",
    "<ol>\n",
    "    <li>Redes Neurais Feed-Forward: São mais comuns, a primeira camada é a entrada e a última camada é a saída, se houver uma camada oculta entre as duas, é chamado de redes neurais profundas(Deep Learning). A rede calcula uma série de transformação que altera a semelhança entre os casos, as atividades dos neurônios em cada camada são uma função não-linear das atividades na camada anterior. </li>\n",
    "    <li>Redes Recorrentes: Essa rede é utilizada quando para se obter o valor de saída atual é necessário analisar o valor do passado. Essa rede é equivalente a redes muito profundas com uma camada oculta por fatia de tempo; exceto que eles usam os mesmos pesos em cada fatia de tempo e recebem entrada em cada fatia. Eles têm a capacidade de lembrar informações em seu estado oculto por um longo período de tempo, mas é muito difícil treiná-las para usar esse potencial. Podem possuir uma dinâmica complicada, sendo difíceis de treinar, mas são mais biologicamente realistas.</li>\n",
    "    <li>Redes Conectadas Simetricamente: São como as redes recorrentes mas elas possuem o mesmo peso em ambas as direções.  As redes conectadas simetricamente sem unidades ocultas são chamadas de “Redes Hopfield”. As redes conectadas simetricamente com unidades ocultas são chamadas de “Máquinas de Boltzmann”. </li>\n",
    "</ol>    \n",
    "\n",
    "O trabalho atual se enquadra na categoria Redes Neurais Feed-Forward, a arquitetura utilizada é a Redes Multilayer Perceptrons (MLP), a rede MLP é composta por mais de um perceptron, e possui uma camada de entrada, uma de saída que toma uma decisão sobre a entrada, e entre as duas pode haver várias camadas ocultas. O MLP é muito utilizado em problemas de aprendizagem supervisionados, ele treina um conjunto de pares entrada-saída e aprende a modelar a correlação entre as entradas e saídas, no treinamento é realizado o ajuste dos parâmetros, pesos e bias da rede para conseguir minimizar o erro.  O backpropagation é usado para fazer os ajustes dos pesos e de bias em relação ao erro, e o próprio erro pode ser medido de várias maneiras, inclusive pelo erro quadrático médio.\n",
    "\n",
    "<h4> Base de dados: Online News Popularity </h4>\n",
    "\n",
    "A base de dados [Online News Popularity](http://archive.ics.uci.edu/ml/datasets/Online+News+Popularity) possui 60 atributos de várias notícias a ser analisados pela rede neural mais 1 atributo que possui o valor alvo, os atributos possuem o seguinte significado:\n",
    "\n",
    "<ol>\n",
    "<li>url: Url da notícia</li>\n",
    "<li>timedelta: Dias entre a publicação da notícia e a aquisição do conjunto de dados (não-preditiva)</li>\n",
    "<li>n_tokens_title: Quantidade de palavras do título</li>\n",
    "<li>n_tokens_content: Quantidade de palavras do conteúdo</li>\n",
    "<li>n_unique_tokens: Quantidade de palavras únicas no conteúdo</li>\n",
    "<li>n_non_stop_words: Taxa de palavras sem parar no conteúdo</li>\n",
    "<li>n_non_stop_unique_tokens: Quantidade de palavras não únicas no conteúdo</li>\n",
    "<li>num_hrefs: Número de links</li>\n",
    "<li>num_self_hrefs: Número de links para outras notícias publicados pela Mashable </li>\n",
    "<li>num_imgs: Número de imagens</li>\n",
    "<li>num_videos: Número de vídeos </li>\n",
    "<li>average_token_length: Tamanho médio das palavras no conteúdo</li>\n",
    "<li>num_keywords: Número de palavras-chave nos metadados</li>\n",
    "<li>data_channel_is_lifestyle: É o canal de dados 'Lifestyle'?</li>\n",
    "<li>data_channel_is_entertainment: O canal de dados é 'Entretenimento'?</li>\n",
    "<li>data_channel_is_bus: É o canal de dados 'Business'?</li>\n",
    "<li>data_channel_is_socmed: É o canal de dados 'Social Media'?</li>\n",
    "<li>data_channel_is_tech: O canal de dados é 'Tech'? </li>\n",
    "<li>data_channel_is_world: é o canal de dados 'World'? </li>\n",
    "<li>kw_min_min: Pior palavra-chave (min. Compartilhamentos)</li>\n",
    "<li>kw_max_min: Pior palavra-chave (máx. Compartilhamentos)</li>\n",
    "<li>kw_avg_min: Pior palavra-chave (média de compartilhamentos)</li>\n",
    "<li>kw_min_max: Melhor palavra-chave (min. Compartilhamentos)</li>\n",
    "<li>kw_max_max: Melhor palavra-chave (máx. Compartilhamentos)</li>\n",
    "<li>kw_avg_max: Melhor palavra-chave (média de compartilhamentos)</li>\n",
    "<li>kw_min_avg: média palavra-chave (min. partes)</li>\n",
    "<li>kw_max_avg: média palavra-chave (máx. compartilhamentos)</li>\n",
    "<li>kw_avg_avg: média palavra-chave (média de compartilhamentos)</li>\n",
    "<li>self_reference_min_shares: minimo de ações de notícias referenciados em Mashable</li>\n",
    "<li>self_reference_max_shares: máx. ações de notícias referenciados em Mashable</li>\n",
    "<li>self_reference_avg_sharess: média. ações de notícias referenciados em Mashable</li>\n",
    "<li>weekday_is_monday: A notícia foi publicado na segunda-feira?</li>\n",
    "<li>weekday_is_tuesday: A notícia foi publicado em uma terça-feira?</li>\n",
    "<li>weekday_is_wednesday: A notícia foi publicado em uma quarta-feira?</li>\n",
    "<li>weekday_is_thursday: A notícia foi publicado em uma quinta-feira?</li>\n",
    "<li>weekday_is_friday: A notícia foi publicado em uma sexta-feira?</li>\n",
    "<li>weekday_is_saturday: A notícia foi publicado em um sábado?</li>\n",
    "<li>weekday_is_sunday: A notícia foi publicado em um domingo?</li>\n",
    "<li>is_weekend: A notícia foi publicado no final de semana?</li>\n",
    "<li>LDA_00: Proximidade do tópico 0 do LDA</li>\n",
    "<li>LDA_01: Proximidade do tema 1 do LDA</li>\n",
    "<li>LDA_02: Proximidade do tópico 2 do LDA</li>\n",
    "<li>LDA_03: Proximidade do tema 3 do LDA</li>\n",
    "<li>LDA_04: Proximidade do tema 4 do LDA</li>\n",
    "    \n",
    "<li>global_subjectivity: Subjetividade do texto</li>\n",
    "<li>global_sentiment_polarity: polaridade do sentimento de texto</li>\n",
    "<li>global_rate_positive_words: Taxa de palavras positivas no conteúdo</li>\n",
    "<li>global_rate_negative_words: Taxa de palavras negativas no conteúdo</li>\n",
    "<li>rate_positive_words: Taxa de palavras positivas entre tokens não neutros</li>\n",
    "<li>rate_negative_words: Taxa de palavras negativas entre tokens não neutros</li>\n",
    "<li>avg_positive_polarity: média polaridade de palavras positivas</li>\n",
    "<li>min_positive_polarity: min. polaridade de palavras positivas</li>\n",
    "<li>max_positive_polarity: máx. polaridade de palavras positivas</li>\n",
    "<li>avg_negative_polarity: média polaridade de palavras negativas</li>\n",
    "    <li>min_negative_polarity: min. polaridade de palavras negativas</li>\n",
    "<li>max_negative_polarity: máx. polaridade de palavras negativas</li>\n",
    "<li>title_subjectivity: subjetividade do título</li>\n",
    "<li>title_sentiment_polarity: polaridade do título</li>\n",
    "<li>abs_title_subjectivity: Nível de subjetividade absoluta</li>\n",
    "<li>abs_title_sentiment_polarity: nível de polaridade absoluta</li>\n",
    "<li>ações: Número de ações (alvo)</li>\n",
    "</ol>\n",
    "\n",
    "Este conjunto de dados resume um conjunto heterogêneo de características sobre artigos publicados pela Mashable em um período de dois anos. O objetivo é prever o número de compartilhamentos nas redes sociais (popularidade). A base de dados contém 39644 exemplos. A média de compartilhamentos é de 3395, assim, valores abaixo dessa média serão considerados não populares, e valores iguais ou acima dessa média serão considerados populares.\n",
    "\n",
    "<h3> Desenvolvimento </h3>\n",
    "Inicialmente será importado as bibliotecas que serão utilizadas no projeto, e será utilizado um código simples do keras de classificação binária, pois o resultado é binário, ou é popular, ou não é.\n",
    "\n",
    "Para separar a base em treino e teste foi utilizado o train_test_split como mostra a linha 21, para teste foi separado 30% da base. Além do teste e do treino, também foi separado 25% do treino para validação, afim de não ter uma base viciada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Elaine\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#importando bibliotecas necessárias no projeto\n",
    "from sklearn import svm\n",
    "from keras.models import Sequential\n",
    "from keras.utils import plot_model\n",
    "from keras import regularizers, optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.python.client import device_lib\n",
    "from sklearn.svm import SVC\n",
    "from keras import utils as np_utils\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = csv.reader(open('OnlineNewsPopularity.csv','r'), delimiter=',') #lendo os atributos da base de dados\n",
    "\n",
    "rows = np.array(list(reader))\n",
    "labels = rows[0] #vetor com os labels das caracteristicas\n",
    "\n",
    "X = rows[1:-1, 1:-1] #vetor de caracteristicas\n",
    "Ya = rows[1:-1, -1] #Vetor de resultados\n",
    "\n",
    "Y = []\n",
    "\n",
    "index = 0\n",
    "\n",
    "for y in Ya:\n",
    "    if(int(y) >= 3395):\n",
    "        Y.insert(index, True)        \n",
    "    else:\n",
    "        Y.insert(index, False)\n",
    "    \n",
    "    index += 1\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, stratify=Y) #separando em um conjunto de treino e outro de teste\n",
    "\n",
    "num_input = x_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Função de ativação </h4>\n",
    "\n",
    "A função de ativação utilizada foi a Relu, ela é a função de ativação mais utilizada em redes neurais hoje em dia. A função é definida como f(x) = max(0, x). A função não é linear, assim, se pode facilmente copiar os erros para trás e ter várias camadas de neurônios ativados pela função Relu. A sua principal vantagem é que ela não ativa todos os neurônios ao mesmo tempo, e a sua desvantagem é que ela pode ter problemas com os gradientes que se deslocam em direção a zero.\n",
    "\n",
    "<h4> Método de descida de gradiente </h4>\n",
    "\n",
    "O método de descida de gradiente utilizado foi o Adam. O Adam é um método para otimização estocástica eficiente que possui pouca exigência de memória, ele calcula as taxas individuais de aprendizagem adaptativa para diferentes parâmetros de estimativas de primeiro e segundo momentos dos gradientes. O método é simples de implementar, é computacionalmente eficiente, tem poucos requisitos de memória, é invariante para o reescalonamento diagonal dos gradientes e é bem adequado para problemas que são grandes em termos de dados e / ou parâmetros. O método também é apropriado para objetivos não estacionários e problemas com gradientes muito ruidosos e / ou esparsos.\n",
    "\n",
    "<h4> Função de custo </h4>\n",
    "A cada iteração a rede neural precisa alterar os valores dos pesos para tentar chegar num resultado melhor, para isso é necessário uma função que nos mostre o quão boa é a solução atual, essa função é chamada de função de custo. A função de custo utilizada é a \"binary_crossentropy\", ela utiliza a medida de entropia cruzada, que é usada como uma medida de erro quando as saídas de uma rede podem ser pensadas como representando hipóteses independentes e as ativações dos nós podem ser entendidas como representando a probabilidade (ou a confiança) que cada uma das hipóteses pode ser verdadeira. A entropia cruzada indica a diferença do valor que é esperado e da saída real. A entropia cruzada é mais útil em problemas cujo objetivos são 0 ou 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Overfitting </h4>\n",
    "O código inicial está representado a seguir demonstra o overfitting . O overfitting ocorre quando utilizamos neurônios igual ou maiores que 96 na segunda camada.\n",
    "\n",
    "Como é observado na figura 6, o overfitting acontece devido ao fato de a rede neural apresentar melhores resultados no treino do que no teste.\n",
    "![código overfit](https://uploaddeimagens.com.br/images/001/743/022/full/overfit.png?1543063949)  <center> *figura 6 - Resultado do overfitting* </center>\n",
    "\n",
    "A figura 7 mostra a rede neural utilizando 90 neurônios, nessa fase não acontece o overfitting, pois o teste obtem melhores resultados que o treino.\n",
    "![código overfit](https://uploaddeimagens.com.br/images/001/743/140/full/overfit2.png?1543073207)  <center> *figura 7 - Resultado do overfitting* </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20812 samples, validate on 6938 samples\n",
      "Epoch 1/10\n",
      "20812/20812 [==============================] - 19s 918us/step - loss: 3.2682 - acc: 0.7972 - val_loss: 3.3337 - val_acc: 0.7932\n",
      "Epoch 2/10\n",
      "17088/20812 [=======================>......] - ETA: 2s - loss: 3.3023 - acc: 0.7951"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-d62b8a2c8b53>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m               metrics=['accuracy'])\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mloss_and_metrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=100, activation='relu', input_dim=num_input))\n",
    "model.add(Dense(units=59, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_split=0.25, epochs=50, batch_size=16, verbose=1)\n",
    "\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=16)\n",
    "print(\"\\n Taxa de acerto: %.2f%%\" % (loss_and_metrics[1]*100))\n",
    "\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Regularização </h4>\n",
    "\n",
    "A regularização pode ajudar a reduzir o overfitting. Analisaremos os efeitos de três técnicas de regularização no \n",
    "código, a técnica L1, L2 e Dropout. A intenção da regularização é fazer com que a rede prefira aprender pesos pequenos. Os pesos grandes só são permitidos se melhorarem bastante a primeira parte da função de custo, ou seja, ela tenta encontrar pequenos pesos e minimizar a função de custo original.\n",
    "\n",
    "Tanto na técnica L1 quanto na L2 o resultado é a diminuição dos valores dos pesos, mas a maneira como os pesos diminuem é diferente. Quando um peso específico tem uma grande magnitude, a regularização L1 reduz o peso muito menos do que a Regularização L2, mas, quando |w| é pequeno, a regularização L1 reduz o peso muito mais do que a regularização L2, assim a regularização L1 tende a concentrar o peso da rede em um número relativamente pequeno de conexões de alta importância, enquanto os outros pesos são direcionados para zero.\n",
    "\n",
    "Configurações utilizadas no modelo de regularização L1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=100, activation='relu', input_dim=num_input, activity_regularizer=regularizers.l1(0.01)))\n",
    "model.add(Dense(units=59, activation='relu', activity_regularizer=regularizers.l1(0.01)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_split=0.25, epochs=50, batch_size=16, verbose=1)\n",
    "\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=16)\n",
    "\n",
    "print(\"\\n Taxa de acerto: %.2f%%\" % (loss_and_metrics[1]*100))\n",
    "\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com essas configurações no L1 foi obtido uma taxa de acerto de 79,63%. A figura 8 demostra o histórico da acurácia para o L1 com valor de 0.1.\n",
    "![l1 - 0,1](https://uploaddeimagens.com.br/images/001/744/765/full/l1-0_01.png?1543200998)  <center> *figura 8 - Histórico de acurácia para L1 = 0.1* </center>\n",
    "\n",
    "A figura 9 demostra o histórico da acurácia para o L1 com valor de  de 0.01 nesse caso o resultado do teste apresentou ser pior do que o utilizado com 0.1, pois a taxa de acerto obtida foi de 79,60%.\n",
    "\n",
    "![l1 - 0,01](https://uploaddeimagens.com.br/images/001/744/780/full/l1-0_01.png?1543201813)  <center> *figura 9 - Histórico de acurácia para L1 = 0.01* </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configurações utilizadas no modelo de regularização L2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=100, activation='relu', input_dim=num_input, kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dense(units=59, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_split=0.25, epochs=50, batch_size=16, verbose=1)\n",
    "\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=16)\n",
    "\n",
    "print(\"\\n Taxa de acerto: %.2f%%\" % (loss_and_metrics[1]*100))\n",
    "\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com essas configurações no L2 foi obtido uma taxa de acerto de 79,62%. A figura 10 demostra o histórico da acurácia para o L1 com valor de 0.01, a média da taxa de acerto de treino é de 79,61% e de teste é de 79,65%.\n",
    "![l1 - 0,01](https://uploaddeimagens.com.br/images/001/744/708/full/l2-0_01.png?1543197697)  <center> *figura 10 - Histórico de acurácia para L2 = 0.01* </center>\n",
    "\n",
    "A figura 11 demostra o histórico da acurácia para o L2 com valor de  de 0.1, nesse caso o resultado do teste apresentou ser praticamente o mesmo que o anterior.\n",
    "\n",
    "![l1 - 0,1](https://uploaddeimagens.com.br/images/001/744/722/full/l2-0_1.png?1543199138)  <center> *figura 11 - Histórico de acurácia para L2 = 0.1* </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Dropout é uma técnica diferente da L1 e L2, ele não depende da modificação da função de custo, ele modifica a própria rede. O Dropout elimina aleatoriamente (e temporariamente) alguns dos neurônios ocultos na rede, mas deixa os neurônios de entrada e saída intocados.\n",
    "\n",
    "Configurações utilizadas no Dropout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=100, activation='relu', input_dim=num_input)\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=59, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_split=0.25, epochs=50, batch_size=16, verbose=1)\n",
    "\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=16)\n",
    "\n",
    "print(\"\\n Taxa de acerto: %.2f%%\" % (loss_and_metrics[1]*100))\n",
    "\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com o Dropout de 0.5 foi obtido uma taxa de acerto de 79,62%, a figura 12 mostra o histórico da acurácia na fase de treino e teste.\n",
    "\n",
    "![drop - 0,5](https://uploaddeimagens.com.br/images/001/744/803/full/drop-0_5.png?1543204505) <center> *figura 12 - Histórico de acurácia para Dropout = 0.5* </center>\n",
    "\n",
    "Com o Dropout de 0.3 foi obtido uma taxa de acerto de 79,62%, a figura 13 mostra o histórico da acurácia na fase de treino e teste.\n",
    "\n",
    "![drop - 0,3](https://uploaddeimagens.com.br/images/001/744/821/full/drop-0_3.png?1543206419) <center> *figura 13 - Histórico de acurácia para Dropout = 0.3* </center>\n",
    "\n",
    "Com o Dropout de 0.7 foi obtido uma taxa de acerto de 79,62%, a figura 14 mostra o histórico da acurácia na fase de treino e teste.\n",
    "\n",
    "![drop - 0,7](https://uploaddeimagens.com.br/images/001/744/819/full/drop-0_7.png?1543206380) <center> *figura 14 - Histórico de acurácia para Dropout = 0.7* </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Hiperparâmetros </h4>\n",
    "\n",
    "Inicialmente será decidido o número de neuronios utilizados na segunda camada oculta, como mostra o código a seguir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=80, activation='relu', input_dim=num_input))\n",
    "model.add(Dense(units=20, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_split=0.25, epochs=5, batch_size=16, verbose=1)\n",
    "\n",
    "print(\"\\n Taxa de acerto: %.2f%%\" % (loss_and_metrics[1]*100))\n",
    "\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A figura 15 mostra o histórico da acurácia obtido, utilizando 20 neuronios na segunda camada oculta. A taxa de acerto foi de 20,37%.\n",
    "\n",
    "![hp - 20](https://uploaddeimagens.com.br/images/001/744/843/full/hp-01.pnh.PNG?1543210289) <center> *figura 15 - Histórico de acurácia para 2² camada oculta = 20* </center>\n",
    "\n",
    "A figura 16 mostra o histórico da acurácia obtido, utilizando 59 neuronios na segunda camada oculta. A taxa de acerto foi de 79,62%. Foi testado até valores maiores, como 200 neuronios, mas apresentou o mesmo resultado, portanto a segunda camada oculta irá utilizar 59 neuronios.\n",
    "\n",
    "![hp - 59](https://uploaddeimagens.com.br/images/001/744/842/full/hp-02.PNG?1543210194) <center> *figura 16 - Histórico de acurácia para 2² camada oculta = 59* </center>\n",
    "\n",
    "Como se pode observar, no primeiro caso ocorre o overfitting, então concluimos que valores baixos na segunda camada oculta não é bom, por isso será utilizado 59 neuronios.\n",
    "\n",
    "Outro hiperparâmetro que pode ser ajustado é quantidade de dados retirados do treino para validação, foi testado utilizar valores de 25%, 30% e 40%, mas não houve variação na taxa de acerto, a mesma se manteve em 79.62% Então será mantido com o valor de 25%. Além da validação foi feito modificações no tamanho do batch, mas a medida que foi aumentando o valor do batch, o mesmo manteve a taxa de acerto, até cair para 20%.\n",
    "\n",
    "\n",
    "<h3> Resultados </h3>\n",
    "O melhor resultado que conseguimos obter foi o de 79,63%. Chegamos a esse resultado utilizando a regularização L1 na rede neural. A rede neural conseguiu obter bons resultados. De acordo com [Kelwin Fernandes\n",
    "](https://pdfs.semanticscholar.org/ad7f/3da7a5d6a1e18cc5a176f18f52687b912fea.pdf), utilizando 70% dos dados para treino e 30% para teste, o KNN e o Naive Bayes obteve uma taxa de acerto de 62%, o SVM e o AdaBoost de 66% e o RF de 67%. Para realizar mais comparações, também foi realizado teste para a base de dados utilizando a Árvore de Decisão, onde a mesma apresentou um resultado melhor, obtendo uma taxa de acerto de 79,66%.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
